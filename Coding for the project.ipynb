{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be413cf",
   "metadata": {},
   "source": [
    "# Exploratory analysis of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce2b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the necessary module\n",
    "import pandas as pd\n",
    "\n",
    "# Reading the Youtube Spam data\n",
    "pre_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Display first 5 rows of the data\n",
    "print(pre_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821da4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the names of the columns\n",
    "colnam = pre_data.columns\n",
    "\n",
    "for x in colnam:\n",
    "    print(f'Colume name: {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the dimensions of the data\n",
    "pre_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f019344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NAs in each columns\n",
    "print('The number of NAs in the data set categorized by columns are: \\n', pre_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicated rows\n",
    "print('The total number of duplicated rows in the data set is:', pre_data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique contributors in the data set\n",
    "pre_data['AUTHOR'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd330a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the necessary module\n",
    "from collections import Counter\n",
    "\n",
    "# Count the top 5 most dedicated contributors\n",
    "top_5_author = Counter(pre_data['AUTHOR']).most_common(5)\n",
    "\n",
    "for x in top_5_author:\n",
    "    print(f'{x[0]}: {x[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada68b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the necessary module\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Visualize top 5 most common contributors in the data set\n",
    "name = [x[0] for x in top_5_author]\n",
    "count = [x[1] for x in top_5_author]\n",
    "\n",
    "plt.bar(name, count, width = 0.4)\n",
    "\n",
    "plt.xlabel('Authors')\n",
    "plt.ylabel('Number of contributions')\n",
    "plt.title('Top 5 most common contributors')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ce658",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = pre_data[pre_data['CLASS'] == 1]\n",
    "\n",
    "top_5_spammer = Counter(filtered_data['AUTHOR']).most_common(5)\n",
    "\n",
    "for x in top_5_spammer:\n",
    "    print(f'{x[0]}: {x[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 5 spammers in the data set\n",
    "name = [x[0] for x in top_5_spammer]\n",
    "count = [x[1] for x in top_5_spammer]\n",
    "\n",
    "plt.bar(name, count, width = 0.4)\n",
    "\n",
    "plt.xlabel('Authors')\n",
    "plt.ylabel('Number of contributions')\n",
    "plt.title('Top 5 spammer')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632526d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nessary module\n",
    "import re\n",
    "\n",
    "# A function to see if a comment contains URL\n",
    "def contains_url(comment):\n",
    "    pattern = r'http[s]?://\\S+|www\\.\\S+'\n",
    "    return bool(re.search(pattern, comment))\n",
    "\n",
    "pre_data['Contains_url'] = pre_data['CONTENT'].apply(contains_url)\n",
    "\n",
    "num_urls = pre_data['Contains_url'].sum()\n",
    "\n",
    "print(f'The number of comments containing URLs is: {num_urls}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_spam = pre_data[(pre_data['CLASS'] == 1) & (pre_data['Contains_url'] == True)]\n",
    "\n",
    "prop_url_spam = len(url_spam) / num_urls * 100\n",
    "\n",
    "print(f'The proposition of spam comments containing a URL is: {prop_url_spam}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f16af3",
   "metadata": {},
   "source": [
    "We can see that as long as the comment is a spam, it is very likely to contain a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0534f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary module\n",
    "import emoji\n",
    "\n",
    "# Function to check if a comment contains emojis\n",
    "def contains_emoji(comment):\n",
    "    return bool(emoji.emoji_count(comment))\n",
    "\n",
    "pre_data['Contains_emo'] = pre_data['CONTENT'].apply(contains_emoji)\n",
    "\n",
    "num_emo = pre_data['Contains_emo'].sum()\n",
    "\n",
    "print(f'The number of comments containing emoji is: {num_emo}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb41fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_spam = pre_data[(pre_data['CLASS'] == 1) & (pre_data['Contains_emo'] == True)]\n",
    "\n",
    "prop_emo_spam = len(emo_spam) / num_emo * 100\n",
    "\n",
    "print(f'The proposition of spam comments containing emoji is: {prop_emo_spam}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341bcb1e",
   "metadata": {},
   "source": [
    "We can see that the not a lot of spam comment contains emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69763e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the spam comment and non-spam comment\n",
    "spam_comment = pre_data[pre_data['CLASS'] == 1]['CONTENT']\n",
    "non_spam_comment = pre_data[pre_data['CLASS'] == 0]['CONTENT']\n",
    "\n",
    "punctuation_pattern = r'[^\\w\\s]'\n",
    "\n",
    "punc_list = []\n",
    "for comment in spam_comment:\n",
    "    comment = comment.replace('\\ufeff', '') # Removing the Byte Order Mark\n",
    "    punctuations = re.findall(punctuation_pattern, comment)\n",
    "    punc_list.extend(punctuations)\n",
    "\n",
    "punctuation_counts = Counter(punc_list)\n",
    "\n",
    "top_5_punc = punctuation_counts.most_common(5)\n",
    "\n",
    "for x in top_5_punc:\n",
    "    print(f'{x[0]}: {x[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe97aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define whether the comment contains top spam punctuations\n",
    "def contains_punc(comment):\n",
    "    tokens = comment.lower().split(' ')\n",
    "    return any(word in top_5_punc for word in tokens)\n",
    "\n",
    "pre_data['Contains_punc'] = pre_data['CONTENT'].apply(contains_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac2d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the necessary modules\n",
    "import nltk # Natural language toolkit\n",
    "from nltk.corpus import stopwords # Importing common stopwords\n",
    "from nltk.tokenize import word_tokenize # Import tokenizer\n",
    "from nltk.stem import WordNetLemmatizer # Import lemmatizer\n",
    "\n",
    "# Download necessary module resources\n",
    "nltk.download('stopwords') # Download stopwords database\n",
    "nltk.download('punkt') # Download Punkt tokenizer\n",
    "nltk.download('wordnet') # Enable the WordNetLemmatizer by downloading WordNet database\n",
    "nltk.download('omw-1.4') # A additional database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Building the text cleaning function and tokenization\n",
    "def clean_text(text):\n",
    "    # Step 1: Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Step 2: Remove emojis\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    \n",
    "    # Step 3: Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 4: Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Step 5: Tokenize\n",
    "    token = word_tokenize(text)\n",
    "    \n",
    "    # Step 6: Remove stopwords\n",
    "    token = [word for word in token if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Step 7: Lemmatize\n",
    "    token = [lemmatizer.lemmatize(word) for word in token]\n",
    "    \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3edeb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count top 5 most commonly used words in spam comment\n",
    "all_tokens = []\n",
    "for comment in spam_comment:\n",
    "    token = clean_text(comment)\n",
    "    for x in token:\n",
    "        all_tokens.append(x)\n",
    "\n",
    "top_5_words = Counter(all_tokens).most_common(5)\n",
    "\n",
    "for x in top_5_words:\n",
    "    print(f'{x[0]}: {x[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bf7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a comment contain those top used spam words\n",
    "def contains_words(comment):\n",
    "    tokens = clean_text(comment)\n",
    "    return any(word in tokens for word in top_5_words)\n",
    "\n",
    "pre_data['Contains_words'] = pre_data['CONTENT'].apply(contains_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde54fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique videos in the data set\n",
    "pre_data['VIDEO_NAME'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b113bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the composition of comments of the videos\n",
    "vid_comment = pre_data['VIDEO_NAME'].value_counts()\n",
    "spam_vid_comment = filtered_data['VIDEO_NAME'].value_counts()\n",
    "\n",
    "total_vid_comment = pd.DataFrame({\n",
    "    'Total comments of a video': vid_comment,\n",
    "    'Total spam comment of a video': spam_vid_comment\n",
    "})\n",
    "\n",
    "total_vid_comment.plot(kind='bar', figsize=(12, 6), stacked=True)\n",
    "\n",
    "plt.title(\"Number of comments by video\")\n",
    "plt.xlabel(\"Video name\")\n",
    "plt.ylabel(\"Comment counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357118f4",
   "metadata": {},
   "source": [
    "### Explore CLASS colume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_comment = pre_data.shape[0]\n",
    "\n",
    "num_spam = len(spam_comment)\n",
    "\n",
    "prop_spam = len(spam_comment) / pre_data.shape[0] * 100\n",
    "\n",
    "print(f'The proposition of spam comment is: {prop_spam}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c3e5a",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff08f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing all columes we have in the dataset so far.\n",
    "\n",
    "colnam = pre_data.columns\n",
    "\n",
    "for x in colnam:\n",
    "    print(f'Colume name: {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pre_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e46cc",
   "metadata": {},
   "source": [
    "## Forward stepwise selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary modules\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorizing the text data in the data set\n",
    "pre_data['text_data'] = pre_data['CONTENT']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(pre_data['text_data'])\n",
    "tfidf_features = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8038db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define all possible predictors, we use dictionary becuase it is easier to present the final feature\n",
    "all_predictors = {\n",
    "    'Content colume': tfidf_features,\n",
    "    'Contains_url colume': pre_data[['Contains_url']],\n",
    "    'Contains_emo colume': pre_data[['Contains_emo']],\n",
    "    'Contains_punc colume': pre_data[['Contains_punc']],\n",
    "    'Contains_words colume': pre_data[['Contains_words']]\n",
    "}\n",
    "\n",
    "# Define the response variable\n",
    "response = pre_data['CLASS']\n",
    "\n",
    "# Define unused predictors\n",
    "unused_predictors = list(all_predictors.keys())\n",
    "\n",
    "# Initialization\n",
    "selected_predictors = []\n",
    "best_accuracy = 0\n",
    "\n",
    "# The adjusted forward stepwise selection\n",
    "while unused_predictors:\n",
    "    temp_best_accuracy = 0\n",
    "    temp_best_predictor = None\n",
    "\n",
    "    for index in unused_predictors:\n",
    "        model = RandomForestClassifier(bootstrap = True, random_state = 123)\n",
    "\n",
    "        temp_predictors = pd.concat([all_predictors[x] for x in selected_predictors + [index]], axis = 1)\n",
    "        \n",
    "        x_train, x_test, y_train, y_test = train_test_split(temp_predictors, response, test_size = 0.3, random_state = 123)\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        y_prediction = model.predict(x_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_prediction)\n",
    "\n",
    "        if temp_best_accuracy < accuracy:\n",
    "            temp_best_accuracy = accuracy\n",
    "            temp_best_predictor = index\n",
    "\n",
    "    if temp_best_predictor != None and best_accuracy < temp_best_accuracy:\n",
    "        selected_predictors.append(temp_best_predictor)\n",
    "        unused_predictors.remove(temp_best_predictor)\n",
    "        best_accuracy = temp_best_accuracy\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e63190",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c994fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5511b30",
   "metadata": {},
   "source": [
    "## Investigate the behavior of the model and its relavent statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c485a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the preliminary model\n",
    "final_predictors = pd.concat([all_predictors[x] for x in selected_predictors], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(final_predictors, response, test_size=0.3, random_state=123)\n",
    "\n",
    "pre_model = RandomForestClassifier(bootstrap=True, random_state=123)\n",
    "\n",
    "pre_model.fit(x_train, y_train)\n",
    "\n",
    "y_prediction = pre_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f3989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the accuracy score\n",
    "accuracy_score(y_test, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c76dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary package\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Investigate F1 score\n",
    "print(classification_report(y_test,y_prediction)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary module\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Investigate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_prediction)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_matrix)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea2629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary package\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# ROC and AUC\n",
    "y_pred_prob = pre_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label = f'ROC curve with AUC = {roc_auc:.3f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ee2f9",
   "metadata": {},
   "source": [
    "# Final model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing final model using the variable selected above\n",
    "final_predictors = pd.concat([all_predictors[x] for x in selected_predictors], axis=1)\n",
    "\n",
    "final_model = RandomForestClassifier(bootstrap=True, random_state=123)\n",
    "\n",
    "final_model.fit(final_predictors, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b5ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the file for kaggle submission with the final model\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "test_data['Contains_url'] = test_data['CONTENT'].apply(contains_url)\n",
    "\n",
    "test_data['Contains_punc'] = test_data['CONTENT'].apply(contains_punc)\n",
    "\n",
    "test_data['Contains_emo'] = test_data['CONTENT'].apply(contains_emoji)\n",
    "\n",
    "test_data['Contains_words'] = test_data['CONTENT'].apply(contains_words)\n",
    "\n",
    "test_tfidf_matrix = vectorizer.transform(test_data['CONTENT'])\n",
    "test_tfidf_features = pd.DataFrame(test_tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "all_test_predictors = {\n",
    "    'Content colume': test_tfidf_features,\n",
    "    'Contains_url colume': test_data[['Contains_url']],\n",
    "    'Contains_emo colume': test_data[['Contains_emo']],\n",
    "    'Contains_punc colume': test_data[['Contains_punc']],\n",
    "    'Contains_words colume': test_data[['Contains_words']]\n",
    "}\n",
    "\n",
    "final_test_predictors = pd.concat([all_test_predictors[x] for x in selected_predictors], axis = 1)\n",
    "\n",
    "new_predictions = final_model.predict(final_test_predictors)\n",
    "\n",
    "output = pd.DataFrame({\n",
    "    'COMMENT_ID': test_data['COMMENT_ID'],\n",
    "    'CLASS':new_predictions\n",
    "})\n",
    "\n",
    "output.to_csv('predictions_test_final.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
